\chapter{Like Programming, Mathematics has a Culture}

\begin{quote}

Mathematics knows no races or geographic boundaries; for mathematics, the
cultural world is one country. \\

--David Hilbert

\end{quote}

Do you remember when you started to really \emph{learn} programming? I do. I
spent two years in high school programming games in Java. Those two years
easily contain the worst and most embarrassing code I have ever written.  My
code absolutely reeked. Hundred-line functions and thousand-line classes, magic
numbers, unreachable blocks of code, ridiculous comments, a complete disregard
for sensible object orientation, and type-coercion that would make your skin
crawl. The code worked, but it was filled with bugs and mishandled edge-cases.
I broke every rule in the book, and for all my shortcomings I considered myself
a hot-shot (at least, among my classmates!). I didn't know how to design
programs, or what made a program ``good,'' other than that it ran and I could
impress my friends with a zombie shooting game.

Even after I started studying software in college, it was another year before I
knew what a stack frame or a register was, another year before I was halfway
competent with a terminal, another year before I appreciated functional
programming, and to this day I \emph{still} have an irrational fear of systems
programming and networking. I built up a base of knowledge over time, with fits
and starts at every step.

In a college class on C++ I was programming a Checkers game, and my task was to
generate a list of legal jump-moves from a given board state. I used a
depth-first search and a few recursive function calls. Once I had something I
was pleased with, I compiled it and ran it on my first non-trivial example. Lo'
and behold (even having followed test-driven development!), a segmentation
fault smacked me in the face. Dozens of test cases and more than twenty hours
of confusion later, I found the error: my recursive call passed a reference
when it should have been passing a pointer. This wasn't a bug in syntax or
semantics---I understood pointers and references well enough---but a design
error. As most programmers can relate, the most aggravating part was that
changing four characters (swapping a few ampersands with asterisks) fixed it.
Twenty hours of work for four characters! Once I begrudgingly verified it
worked, I promptly took the rest of the day off to play Starcraft.

Such drama is the seasoning that makes a strong programmer. One must study the
topics incrementally, learn from a menagerie of mistakes, and spend hours in a
befuddled stupor before becoming ``experienced.'' This gives rise to all sorts
of programmer culture, Unix jokes, urban legends, horror stories, and reverence
for the masters of C that make the programming community so lovely. It's like a
secret club where you know all the handshakes, but should you forget one, a
crafty use of \texttt{grep} and \texttt{sed} will suffice. The struggle makes
you appreciate the power of debugging tools, slick frameworks, historically
enshrined hacks, and new language features that stop you from shooting your own
foot.

When programmers turn to mathematics, they seem to forget these trials. The
same people who invested years grokking the tools of their trade treat new
mathematical tools and paradigms with surprising impatience. I can see a few
reasons why. One is that they've been taking classes called ``mathematics'' for
far longer than they've been learning to program (and mathematics was always
easy!). The forced prior investment of schooling engenders a certain
expectation. The problem is that the culture of mathematics and the culture of
mathematics education---elementary through lower-level college courses---are
completely different.

Even math majors have to reconcile this. I've had many conversations with such
students, many of whom are friends, colleagues, and even family, who by their
third year decided they didn't really enjoy math. The story often goes like
this: a student who was good at math in high school (perhaps because of its
rigid structure) reaches the point of a math major at which they must read and
write proofs in earnest. It requires an earnest, open-ended exploration that
they don't enjoy. Despite being a stark departure from high school math,
incoming students are never warned in advance. After coming to terms with their
unfortunate situation, they decide that their best option is to hold on until
they can return to the comfortable setting of their prior experiences, this
time in the teacher's chair.

I don't mean to insult teaching as a profession---I love teaching and
understand why one would choose to do it full time. There are many excellent
teachers who excel at both the math and the trickier task of engaging aloof
teenagers to think critically about it. But this pattern of disenchantment
among math teachers is prevalent, and it widens the conceptual gap between
secondary and ``college level'' mathematics. Programmers often have similar
feelings, that the math they were once so good at is suddenly impenetrable.
It's not a feature of math, but a bug in the education system (and a negative
feedback loop!) that gets blamed on math as a subject.

Another reason programmers feel impatient is because they do so many things
that relate to mathematics in deep ways. They use graph theory for data
structures and search. They study enough calculus to make video games. They
hear about the Curry-Howard correspondence between proofs and programs. They
hear that Haskell is based on a complicated math thing called category theory.
They even use mathematical results in an interesting way. I worked at a
``blockchain'' company that implemented a Bitcoin wallet, which is based on
elliptic curve cryptography. The wallet worked, but the implementer didn't
understand why. They simply adapted pseudocode found on the internet. At the
risk of a dubious analogy, it's akin to a ``script kiddie'' who uses hacking
tools as black boxes, but has little idea how they work. Mathematicians are on
the other end of the spectrum, caring almost exclusively about why things work
the way they do.

While there's nothing inherently wrong with using mathematics as a black box,
especially the sort of applied mathematics that comes with provable guarantees,
many programmers \emph{want} to understand why they work.  This isn't
surprising, given how much time engineers spend studying source code and the
internals of brittle, technical systems. Systems that programmers rely on, such
as dependency management, load balancers, search engines, alerting systems, and
machine learning, all have rich mathematical foundations.  We're naturally
curious about how they work and how to adapt them to our needs.

Yet another hindrance to mathematics is that it has no centralized
documentation. Instead it has a collection of books, papers, journals, and
conferences, each with discrepancies of presentation, citing each other in a
haphazard manner. A theorem presented at a computer science conference can be
phrased in completely unfamiliar terms in a dynamical systems journal---even
though they boil down to the same facts!  In subfields like network science
that straddle disciplines, one often sees ``translation tables'' for jargon.

Dealing with this is not easy. Students of mathematics solve these problems
with knowledgeable teachers. Working mathematicians just ``do it.'' They work
out the translation details themselves with coffee and contemplation. Advanced
books also lean toward terseness, despite being titled as ``elementary'' or an
``introduction.'' They opt not to redefine what they think the reader must
already know. The purest fields of mathematics take a sort of pretentious
pride in how abstract and compact their work is (to the point where many
students spend weeks or months understanding a single chapter!).

What programmers would consider ``sloppy'' notation is one symptom of the
problem, but there there are other expectations on the reader that, for better
or worse, decelerate the pace of reading.  Unfortunately I have no solution
here. Part of the power and expressiveness of mathematics is the ability for
its practitioners to overload, redefine, and omit in a suggestive manner.
Mathematicians also have thousands of years of ``legacy'' math that require
backward compatibility. Enforcing a single specification for all of
mathematics---a suggestion I frequently hear from software engineers---would be
horrendously counterproductive.

Indeed, ideas we take for granted today, such as algebraic notation, drawing
functions in the Euclidean plane, and summation notation, were at one point
actively developed technologies. Each of these notations had a revolutionary
effect, not just on science, but also, to quote Bret Victor, on our capacity to
``think new thoughts.''  One can even draw a line from the proliferation of
algebraic notation and the computational questions it raised to the invention
of the computer.\footnote{Leibniz, one of the inventors of calculus, dreamed of
a machine that could automatically solve mathematical problems. Ada Lovelace
(up to some irrelevant debate) designed the first program for computing
Bernoulli numbers, which arise in algebraic formulas for computing sums of
powers of integers. In the early 1900's Hilbert posed his Tenth Problem on
algorithms for computing solutions to Diophantine equations, and later his
Entscheidungsproblem, which was solved concurrently by Church and Turing and
directly led to Turing's code-breaking computer.} Borrowing software
terminology, algebraic notation is among the most influential and scalable
technologies humanity has ever invented.  And as we'll see in
Chapter~\ref{ch:linearalgebra} and Chapter~\ref{ch:groups}, we can find
algebraic structure hiding in exciting places. Algebraic notation helps us
understand this structure not only because we can compute, but also because we
can visually see the symmetries in the formulas. This makes it easier for us to
identify, analyze, and encapsulate structure when it occurs.

Finally, the best mathematicians study concepts that connect decades of
material, while simultaneously inventing new concepts which have no existing
words to describe them.  Without flexible expression, such work would be
impossible. It reduces cognitive load, a theme that will follow us throughout
the book. Unfortunately, it only does so for the readers who have
\emph{already} absorbed the basic concepts of discussion. By contrast, good
software practice encourages code that is simple enough for anyone to
understand. As such, the uninitiated programmer often has a much larger
cognitive load when reading math than when reading a program.

Taken together, mathematical notation is closer to spoken language than to
code. It can reduce one's mental burden via rigorous rules applied to an
external representation, coupled with context and convention. All of this, the
notation, the differences among subfields, the tradeoff between expressiveness
and cognitive load, has grown out of hundreds of years of mathematical
progress.

Equipped with this understanding, that mathematics has culturally relevant
reasons for its strange practices, let's begin our journey through the mists of
math with renewed openness.

Read on, and welcome to the club.
